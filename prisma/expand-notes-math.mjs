import { PrismaClient } from '@prisma/client'

const prisma = new PrismaClient()

// Note updates: { id: string, content: string } for existing notes
// New notes: { taskTitle: string, title: string, content: string } for new notes

const NOTE_UPDATES = [
  {
    id: 'cml9s5ebn000nt4xixqcfh5zq',
    content: `## çŸ©é˜µåˆ†è§£æ€»è§ˆ

çŸ©é˜µåˆ†è§£æ˜¯çº¿æ€§ä»£æ•°ä¸­æœ€å¼ºå¤§çš„å·¥å…·ä¹‹ä¸€ï¼Œå®ƒå°†ä¸€ä¸ªå¤æ‚çš„çŸ©é˜µæ‹†è§£ä¸ºè‹¥å¹²ç»“æ„æ›´ç®€å•çš„çŸ©é˜µä¹‹ç§¯ï¼Œä»è€Œæ­ç¤ºçŸ©é˜µçš„å†…åœ¨ç»“æ„ã€‚åœ¨ ML/AI ä¸­ï¼Œå‡ ä¹æ‰€æœ‰æ ¸å¿ƒç®—æ³•éƒ½ç¦»ä¸å¼€æŸç§å½¢å¼çš„çŸ©é˜µåˆ†è§£ã€‚

### åˆ†è§£æ–¹æ³•è¯¦ç»†å¯¹æ¯”

| åˆ†è§£ | å½¢å¼ | è¦æ±‚ | è®¡ç®—å¤æ‚åº¦ | ä¸»è¦ç”¨é€” |
|------|------|------|-----------|---------|
| EVD | $A = Q\\Lambda Q^{-1}$ | æ–¹é˜µï¼Œéœ€å¯å¯¹è§’åŒ– | O(nÂ³) | ç‰¹å¾åˆ†æã€PCAã€åŠ¨åŠ›ç³»ç»Ÿ |
| SVD | $A = U\\Sigma V^T$ | ä»»æ„çŸ©é˜µ | O(mnÂ²) | é™ç»´ã€æ¨èã€å‹ç¼©ã€ä¼ªé€† |
| QR | $A = QR$ | ä»»æ„çŸ©é˜µ | O(mnÂ²) | æœ€å°äºŒä¹˜ã€æ•°å€¼ç¨³å®šæ±‚è§£ |
| Cholesky | $A = LL^T$ | å¯¹ç§°æ­£å®š | O(nÂ³/3) | é«˜æ–¯è¿‡ç¨‹ã€æ­£å®šçŸ©é˜µæ±‚è§£ |
| LU | $A = LU$ | æ–¹é˜µ | O(nÂ³/3) | çº¿æ€§æ–¹ç¨‹ç»„æ±‚è§£ |
| NMF | $A \\approx WH$ | éè´ŸçŸ©é˜µ | è¿­ä»£æ³• | ä¸»é¢˜æ¨¡å‹ã€å›¾åƒç‰¹å¾ |

### SVD æ·±å…¥ç†è§£

SVD æ˜¯æœ€é€šç”¨çš„çŸ©é˜µåˆ†è§£ï¼Œå¯¹**ä»»æ„** mÃ—n çŸ©é˜µéƒ½é€‚ç”¨ï¼š

$$A = U \\Sigma V^T$$

- $U$ (mÃ—m): å·¦å¥‡å¼‚å‘é‡çŸ©é˜µ â€” $AA^T$ çš„ç‰¹å¾å‘é‡
- $\\Sigma$ (mÃ—n): å¯¹è§’çŸ©é˜µï¼Œå¥‡å¼‚å€¼ $\\sigma_1 \\geq \\sigma_2 \\geq ... \\geq 0$
- $V$ (nÃ—n): å³å¥‡å¼‚å‘é‡çŸ©é˜µ â€” $A^TA$ çš„ç‰¹å¾å‘é‡

**å‡ ä½•ç›´è§‰**: ä»»ä½•çº¿æ€§å˜æ¢ = æ—‹è½¬($V^T$) â†’ ç¼©æ”¾($\\Sigma$) â†’ æ—‹è½¬($U$)

### SVD åœ¨ ML ä¸­çš„æ ¸å¿ƒåº”ç”¨

**1. PCA (ä¸»æˆåˆ†åˆ†æ)**
\`\`\`
X_centered = X - mean(X)           # ä¸­å¿ƒåŒ–
U, Î£, V^T = SVD(X_centered)       # åš SVD
X_reduced = X_centered Â· V[:, :k]  # å–å‰ k ä¸ªä¸»æˆåˆ†

æ–¹å·®è§£é‡Šæ¯”: explained_ratio_i = Ïƒ_iÂ² / Î£Ïƒ_jÂ²
é€‰æ‹© k: ä½¿ç´¯è®¡æ–¹å·®è§£é‡Šæ¯” â‰¥ 95%
\`\`\`

**2. æ¨èç³»ç»Ÿ (çŸ©é˜µè¡¥å…¨)**
\`\`\`
R â‰ˆ U_k Î£_k V_k^T    # ç”¨æˆ·-ç‰©å“è¯„åˆ†çŸ©é˜µçš„ä½ç§©è¿‘ä¼¼

ç”¨æˆ· u å¯¹ç‰©å“ i çš„é¢„æµ‹è¯„åˆ†:
rÌ‚_ui = (U_k Î£_k)[u, :] Â· (V_k)[i, :]^T

Netflix Prize çš„æ ¸å¿ƒæ–¹æ³•ä¹‹ä¸€
\`\`\`

**3. æ–‡æœ¬åˆ†æ (LSA/LSI)**
\`\`\`
TF-IDF çŸ©é˜µ A (terms Ã— documents)
A â‰ˆ U_k Î£_k V_k^T

U_k çš„è¡Œ: è¯åœ¨è¯­ä¹‰ç©ºé—´ä¸­çš„è¡¨ç¤º
V_k çš„è¡Œ: æ–‡æ¡£åœ¨è¯­ä¹‰ç©ºé—´ä¸­çš„è¡¨ç¤º
è¯­ä¹‰ç›¸ä¼¼åº¦: åœ¨ä½ç»´ç©ºé—´ä¸­è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
\`\`\`

**4. å›¾åƒå‹ç¼©**
\`\`\`
åŸå§‹å›¾åƒ A (mÃ—n): å­˜å‚¨ mÃ—n ä¸ªå€¼
ç§©-k è¿‘ä¼¼: å­˜å‚¨ mÃ—k + k + kÃ—n ä¸ªå€¼
å‹ç¼©æ¯”: mn / (mk + k + kn)

ä¾‹: 1000Ã—1000 å›¾åƒ, k=50
åŸå§‹: 1,000,000 å€¼ â†’ å‹ç¼©: 100,050 å€¼ (10x å‹ç¼©)
\`\`\`

### EVD ä¸ SVD çš„å…³ç³»

| æ€§è´¨ | EVD | SVD |
|------|-----|-----|
| é€‚ç”¨èŒƒå›´ | ä»…æ–¹é˜µ | ä»»æ„çŸ©é˜µ |
| åˆ†è§£å”¯ä¸€æ€§ | ä¸å”¯ä¸€ï¼ˆç‰¹å¾å‘é‡æ–¹å‘ï¼‰ | å¥‡å¼‚å€¼å”¯ä¸€ |
| æ•°å€¼ç¨³å®šæ€§ | è¾ƒå·® | ä¼˜ç§€ |
| ç‰¹æ®Šå…³ç³» | $A = Q\\Lambda Q^{-1}$ | å¯¹ç§°çŸ©é˜µ: SVD = EVD |

**å…³é”®è”ç³»**: $\\sigma_i = \\sqrt{\\lambda_i(A^TA)}$

### å…³é”®æ€§è´¨æ€»ç»“

- æ­£äº¤çŸ©é˜µ $Q^TQ = I$ï¼Œä¿æŒå‘é‡é•¿åº¦å’Œè§’åº¦
- $\\text{tr}(A) = \\sum \\lambda_i$ï¼ˆè¿¹ = ç‰¹å¾å€¼ä¹‹å’Œï¼‰
- $\\det(A) = \\prod \\lambda_i$ï¼ˆè¡Œåˆ—å¼ = ç‰¹å¾å€¼ä¹‹ç§¯ï¼‰
- çŸ©é˜µçš„ç§© = éé›¶å¥‡å¼‚å€¼çš„ä¸ªæ•°
- Eckart-Young å®šç†: æˆªæ–­ SVD æ˜¯æœ€ä¼˜ä½ç§©è¿‘ä¼¼ï¼ˆFrobenius èŒƒæ•°ä¸‹ï¼‰
- æ¡ä»¶æ•° $\\kappa(A) = \\sigma_{max} / \\sigma_{min}$ï¼Œè¡¡é‡æ•°å€¼ç¨³å®šæ€§`,
  },
  {
    id: 'cml9s5ebn000pt4xi0d6hneks',
    content: `## çŸ©é˜µå¾®ç§¯åˆ† â€” åå‘ä¼ æ’­çš„æ•°å­¦åŸºç¡€

çŸ©é˜µå¾®ç§¯åˆ†æ˜¯ç†è§£æ·±åº¦å­¦ä¹ ä¸­åå‘ä¼ æ’­ç®—æ³•çš„æ•°å­¦åŸºçŸ³ã€‚æŒæ¡å‘é‡/çŸ©é˜µçš„æ±‚å¯¼è§„åˆ™ï¼Œæ‰èƒ½çœŸæ­£ç†è§£æ¢¯åº¦æ˜¯å¦‚ä½•åœ¨ç¥ç»ç½‘ç»œä¸­æµåŠ¨çš„ã€‚

### å¸ƒå±€çº¦å®š

çŸ©é˜µå¾®ç§¯åˆ†æœ‰ä¸¤ç§å¸ƒå±€çº¦å®šï¼Œæ··æ·†å®ƒä»¬æ˜¯æœ€å¸¸è§çš„é”™è¯¯æ¥æºï¼š

| çº¦å®š | $\\partial y / \\partial \\mathbf{x}$ | å½¢çŠ¶ |
|------|------|------|
| åˆ†å­å¸ƒå±€ | $y$ åœ¨è¡Œï¼Œ$\\mathbf{x}$ åœ¨åˆ— | ä¸ $\\mathbf{x}^T$ åŒå½¢ |
| åˆ†æ¯å¸ƒå±€ | $\\mathbf{x}$ åœ¨è¡Œï¼Œ$y$ åœ¨åˆ— | ä¸ $\\mathbf{x}$ åŒå½¢ |

> PyTorch ç­‰æ·±åº¦å­¦ä¹ æ¡†æ¶ä½¿ç”¨**åˆ†æ¯å¸ƒå±€**ï¼šæ¢¯åº¦ä¸å‚æ•°åŒå½¢ã€‚

### æ ‡é‡å¯¹å‘é‡æ±‚å¯¼

$$\\frac{\\partial}{\\partial \\mathbf{x}} \\mathbf{a}^T\\mathbf{x} = \\mathbf{a}$$

$$\\frac{\\partial}{\\partial \\mathbf{x}} \\mathbf{x}^T\\mathbf{x} = 2\\mathbf{x}$$

$$\\frac{\\partial}{\\partial \\mathbf{x}} \\mathbf{x}^TA\\mathbf{x} = (A + A^T)\\mathbf{x}$$

å½“ $A$ å¯¹ç§°æ—¶ç®€åŒ–ä¸º $2A\\mathbf{x}$ã€‚

### å‘é‡å¯¹å‘é‡æ±‚å¯¼ (Jacobian)

$$\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} = J = \\begin{bmatrix} \\frac{\\partial y_1}{\\partial x_1} & \\cdots & \\frac{\\partial y_1}{\\partial x_n} \\\\ \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial y_m}{\\partial x_1} & \\cdots & \\frac{\\partial y_m}{\\partial x_n} \\end{bmatrix}$$

Jacobian çŸ©é˜µçš„å½¢çŠ¶ä¸º $m \\times n$ï¼ˆ$\\mathbf{y} \\in \\mathbb{R}^m, \\mathbf{x} \\in \\mathbb{R}^n$ï¼‰ã€‚

### é“¾å¼æ³•åˆ™ï¼ˆå‘é‡å½¢å¼ï¼‰

$$\\frac{\\partial L}{\\partial \\mathbf{x}} = \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}^T \\frac{\\partial L}{\\partial \\mathbf{y}} = J^T \\frac{\\partial L}{\\partial \\mathbf{y}}$$

è¿™å°±æ˜¯åå‘ä¼ æ’­ä¸­ **Vector-Jacobian Product (VJP)** çš„æ•°å­¦æœ¬è´¨ã€‚

### åœ¨ç¥ç»ç½‘ç»œå„å±‚ä¸­çš„åº”ç”¨

**çº¿æ€§å±‚**: $\\mathbf{y} = W\\mathbf{x} + \\mathbf{b}$

\`\`\`
âˆ‚L/âˆ‚W = (âˆ‚L/âˆ‚y) Â· x^T          å½¢çŠ¶: (out, in)
âˆ‚L/âˆ‚x = W^T Â· (âˆ‚L/âˆ‚y)          å½¢çŠ¶: (in,)
âˆ‚L/âˆ‚b = âˆ‚L/âˆ‚y                   å½¢çŠ¶: (out,)
\`\`\`

**ReLU å±‚**: $\\mathbf{y} = \\max(0, \\mathbf{x})$

\`\`\`
âˆ‚L/âˆ‚x = (âˆ‚L/âˆ‚y) âŠ™ ğŸ™(x > 0)    å…ƒç´ çº§ä¹˜æ³•
\`\`\`

**Softmax + Cross-Entropy å±‚**:

\`\`\`
p = softmax(z)
L = -Î£ y_i log(p_i)

âˆ‚L/âˆ‚z = p - y                   æœ€ç»ˆå½¢å¼éå¸¸ç®€æ´ï¼

æ¨å¯¼è¿‡ç¨‹:
Jacobian of softmax: âˆ‚p_i/âˆ‚z_j = p_i(Î´_ij - p_j)
ä¸ cross-entropy ç»„åˆåå¤§å¹…ç®€åŒ–
\`\`\`

**æ‰¹é‡çŸ©é˜µå½¢å¼** (batch size = N):

\`\`\`
Y = XW^T + b       # X: (N, d_in), W: (d_out, d_in)
âˆ‚L/âˆ‚W = (âˆ‚L/âˆ‚Y)^T Â· X    å½¢çŠ¶: (d_out, d_in)
âˆ‚L/âˆ‚X = (âˆ‚L/âˆ‚Y) Â· W      å½¢çŠ¶: (N, d_in)
\`\`\`

### å¸¸ç”¨çŸ©é˜µå¾®ç§¯åˆ†æ’ç­‰å¼é€ŸæŸ¥

| è¡¨è¾¾å¼ | å¯¼æ•° (åˆ†æ¯å¸ƒå±€) |
|--------|----------------|
| $\\mathbf{a}^T\\mathbf{x}$ | $\\mathbf{a}$ |
| $\\mathbf{x}^TA\\mathbf{x}$ | $(A+A^T)\\mathbf{x}$ |
| $\\|\\mathbf{x}\\|^2$ | $2\\mathbf{x}$ |
| $\\text{tr}(AXB)$ | $A^TB^T$ |
| $\\text{tr}(X^TAX)$ | $(A+A^T)X$ |
| $\\log\\det(X)$ | $X^{-T}$ |

### è‡ªåŠ¨å¾®åˆ† vs æ‰‹åŠ¨æ¨å¯¼

\`\`\`
å‰å‘æ¨¡å¼ AD:  è®¡ç®— Jacobian-vector product (JVP)
              é€‚åˆ: è¾“å…¥å°‘ã€è¾“å‡ºå¤š

åå‘æ¨¡å¼ AD:  è®¡ç®— vector-Jacobian product (VJP)
              é€‚åˆ: è¾“å…¥å¤šã€è¾“å‡ºå°‘ â† æ·±åº¦å­¦ä¹ çš„æƒ…å†µï¼

PyTorch çš„ autograd = åå‘æ¨¡å¼è‡ªåŠ¨å¾®åˆ†
\`\`\`

> ç†è§£çŸ©é˜µå¾®ç§¯åˆ†ä¸æ˜¯ä¸ºäº†æ‰‹åŠ¨æ¨å¯¼æ¢¯åº¦ï¼ˆæ¡†æ¶ä¼šè‡ªåŠ¨å®Œæˆï¼‰ï¼Œè€Œæ˜¯ä¸ºäº†ç†è§£æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸ã€è®¾è®¡æ–°çš„å±‚ã€è°ƒè¯•è®­ç»ƒé—®é¢˜ã€‚`,
  },
  {
    id: 'cml9s5ebo000ut4xioz0iayus',
    content: `## ML ä¸­å¸¸ç”¨æ¦‚ç‡åˆ†å¸ƒ

æ¦‚ç‡åˆ†å¸ƒæ˜¯æœºå™¨å­¦ä¹ çš„æ•°å­¦è¯­è¨€ã€‚æ¯ä¸ª ML æ¨¡å‹æœ¬è´¨ä¸Šéƒ½åœ¨å¯¹æ•°æ®çš„æŸç§æ¦‚ç‡åˆ†å¸ƒåšå‡è®¾ã€‚ç†è§£è¿™äº›åˆ†å¸ƒçš„æ€§è´¨ã€é€‚ç”¨åœºæ™¯å’Œå®ƒä»¬ä¹‹é—´çš„å…³ç³»ï¼Œæ˜¯æ·±å…¥ç†è§£ ML æ–¹æ³•è®ºçš„åŸºç¡€ã€‚

### ç¦»æ•£åˆ†å¸ƒ

| åˆ†å¸ƒ | å‚æ•° | PMF | æœŸæœ› | æ–¹å·® | ML ç”¨é€” |
|------|------|-----|------|------|---------|
| Bernoulli(p) | p âˆˆ [0,1] | $p^x(1-p)^{1-x}$ | p | p(1-p) | äºŒåˆ†ç±»è¾“å‡º |
| Binomial(n,p) | n âˆˆ â„•, p âˆˆ [0,1] | $\\binom{n}{k}p^k(1-p)^{n-k}$ | np | np(1-p) | n æ¬¡äºŒåˆ†ç±» |
| Categorical(p) | p å‘é‡ | $\\prod p_i^{x_i}$ | - | - | å¤šåˆ†ç±»è¾“å‡º |
| Poisson(Î») | Î» > 0 | $e^{-\\lambda}\\lambda^k/k!$ | Î» | Î» | äº‹ä»¶è®¡æ•°ã€æ–‡æœ¬è¯é¢‘ |
| Geometric(p) | p âˆˆ (0,1] | $(1-p)^{k-1}p$ | 1/p | (1-p)/pÂ² | é¦–æ¬¡æˆåŠŸ |

### è¿ç»­åˆ†å¸ƒ

| åˆ†å¸ƒ | å‚æ•° | PDF æ ¸å¿ƒ | æœŸæœ› | æ–¹å·® | ML ç”¨é€” |
|------|------|---------|------|------|---------|
| Gaussian(Î¼,ÏƒÂ²) | Î¼ âˆˆ â„, ÏƒÂ² > 0 | $\\exp(-\\frac{(x-\\mu)^2}{2\\sigma^2})$ | Î¼ | ÏƒÂ² | å‡ ä¹æ‰€æœ‰è¿ç»­å»ºæ¨¡ |
| Multivariate Normal | Î¼, Î£ | $\\exp(-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu))$ | Î¼ | Î£ | é«˜ç»´è¿ç»­æ•°æ® |
| Uniform(a,b) | a < b | $\\frac{1}{b-a}$ | (a+b)/2 | (b-a)Â²/12 | éšæœºåˆå§‹åŒ– |
| Exponential(Î») | Î» > 0 | $\\lambda e^{-\\lambda x}$ | 1/Î» | 1/Î»Â² | äº‹ä»¶é—´éš” |
| Beta(Î±,Î²) | Î±,Î² > 0 | $x^{\\alpha-1}(1-x)^{\\beta-1}$ | Î±/(Î±+Î²) | å¤æ‚ | æ¦‚ç‡çš„å…ˆéªŒ |
| Gamma(Î±,Î²) | Î±,Î² > 0 | $x^{\\alpha-1}e^{-\\beta x}$ | Î±/Î² | Î±/Î²Â² | ç²¾åº¦/æ–¹å·®çš„å…ˆéªŒ |
| Dirichlet(Î±) | Î± å‘é‡ | $\\prod x_i^{\\alpha_i-1}$ | Î±áµ¢/Î±â‚€ | å¤æ‚ | å¤šåˆ†ç±»æ¦‚ç‡å…ˆéªŒ |

### åˆ†å¸ƒä¹‹é—´çš„å…³ç³»å›¾è°±

\`\`\`
Bernoulli â”€â”€(næ¬¡)â”€â”€â†’ Binomial â”€â”€(nâ†’âˆ,pâ†’0)â”€â”€â†’ Poisson
    |                    |
  (å…ˆéªŒ)              (nâ†’âˆ)
    â†“                    â†“
  Beta              Gaussian (ä¸­å¿ƒæé™å®šç†)
    |                    |
  (å¤šç»´)              (å¤šç»´)
    â†“                    â†“
 Dirichlet      Multivariate Normal
\`\`\`

### é«˜æ–¯åˆ†å¸ƒä¸ºä½•å¦‚æ­¤é‡è¦

1. **ä¸­å¿ƒæé™å®šç†**: å¤§é‡ç‹¬ç«‹éšæœºå˜é‡ä¹‹å’Œè¶‹è¿‘æ­£æ€åˆ†å¸ƒ
2. **æœ€å¤§ç†µ**: åœ¨ç»™å®šå‡å€¼å’Œæ–¹å·®çš„çº¦æŸä¸‹ï¼Œé«˜æ–¯åˆ†å¸ƒçš„ç†µæœ€å¤§
3. **æ•°å­¦ä¾¿åˆ©**: å…±è½­å…ˆéªŒã€çº¿æ€§å˜æ¢ä»ä¸ºé«˜æ–¯
4. **è‡ªç„¶ç•Œæ™®é**: æµ‹é‡è¯¯å·®ã€ç‰©ç†é‡æ³¢åŠ¨

### å…±è½­å…ˆéªŒè¯¦è§£

å…ˆéªŒå’ŒåéªŒå±äºåŒä¸€åˆ†å¸ƒæ—ï¼Œä¾¿äºè´å¶æ–¯æ›´æ–°ï¼Œæ— éœ€å¤æ‚ç§¯åˆ†ï¼š

| ä¼¼ç„¶ | å…ˆéªŒ | åéªŒ | æ›´æ–°è§„åˆ™ |
|------|------|------|---------|
| Bernoulli/Binomial | Beta(Î±,Î²) | Beta(Î±+k, Î²+n-k) | k æ¬¡æˆåŠŸï¼Œn-k æ¬¡å¤±è´¥ |
| Gaussian (å·²çŸ¥ÏƒÂ²) | Gaussian(Î¼â‚€,Ïƒâ‚€Â²) | Gaussian(Î¼â‚™,Ïƒâ‚™Â²) | ç²¾åº¦ç›¸åŠ  |
| Multinomial | Dirichlet(Î±) | Dirichlet(Î±+counts) | å„ç±»åˆ«è®¡æ•° |
| Poisson | Gamma(Î±,Î²) | Gamma(Î±+Î£x, Î²+n) | è§‚æµ‹æ€»æ•° |
| Gaussian (å·²çŸ¥Î¼) | Inverse-Gamma | Inverse-Gamma | æ›´æ–°æ–¹å·® |

\`\`\`
Beta åéªŒæ›´æ–°ç¤ºä¾‹:
å…ˆéªŒ: Beta(2, 2)       # æ¸©å’Œå…ˆéªŒï¼Œå¯¹ç§°
æ•°æ®: 7 æ¬¡æ­£é¢, 3 æ¬¡åé¢
åéªŒ: Beta(2+7, 2+3) = Beta(9, 5)
åéªŒå‡å€¼: 9/14 â‰ˆ 0.643
\`\`\`

### MLE vs MAP vs å…¨è´å¶æ–¯

\`\`\`
MLE (æœ€å¤§ä¼¼ç„¶ä¼°è®¡):
  Î¸* = argmax P(D|Î¸)
  = argmax Î£ log P(x_i|Î¸)
  çº¯æ•°æ®é©±åŠ¨ï¼Œä¸è€ƒè™‘å…ˆéªŒ
  æ•°æ®å°‘æ—¶å®¹æ˜“è¿‡æ‹Ÿåˆ

MAP (æœ€å¤§åéªŒä¼°è®¡):
  Î¸* = argmax P(Î¸|D) = argmax P(D|Î¸)Â·P(Î¸)
  = argmax [Î£ log P(x_i|Î¸) + log P(Î¸)]
  åŠ å…¥å…ˆéªŒæ­£åˆ™åŒ–

  å½“å…ˆéªŒæ˜¯ Gaussian N(0, ÏƒÂ²) â†’ MAP = L2 æ­£åˆ™åŒ– (Ridge)
  å½“å…ˆéªŒæ˜¯ Laplace(0, b)    â†’ MAP = L1 æ­£åˆ™åŒ– (Lasso)

å…¨è´å¶æ–¯ (Full Bayesian):
  P(Î¸|D) = P(D|Î¸)Â·P(Î¸) / P(D)
  ä¸æ±‚ç‚¹ä¼°è®¡ï¼Œä¿ç•™å®Œæ•´åéªŒåˆ†å¸ƒ
  é¢„æµ‹: P(x*|D) = âˆ« P(x*|Î¸)P(Î¸|D)dÎ¸
  è®¡ç®—é€šå¸¸ä¸å¯è¡Œ â†’ MCMC / å˜åˆ†æ¨æ–­è¿‘ä¼¼
\`\`\`

### å¤§æ•°å®šå¾‹ä¸ä¸­å¿ƒæé™å®šç†

\`\`\`
å¤§æ•°å®šå¾‹ (LLN):
  XÌ„_n = (1/n)Î£ X_i â†’ Î¼    (n â†’ âˆ)
  æ ·æœ¬å‡å€¼æ”¶æ•›åˆ°æ€»ä½“å‡å€¼
  å¼± LLN: ä¾æ¦‚ç‡æ”¶æ•›
  å¼º LLN: å‡ ä¹å¿…ç„¶æ”¶æ•›

ä¸­å¿ƒæé™å®šç† (CLT):
  âˆšn(XÌ„_n - Î¼) / Ïƒ â†’ N(0, 1)    (n â†’ âˆ)
  ä¸ç®¡åŸå§‹åˆ†å¸ƒæ˜¯ä»€ä¹ˆï¼
  å‰æ: ç‹¬ç«‹åŒåˆ†å¸ƒï¼Œæœ‰é™æ–¹å·®

  å®é™…æ„ä¹‰:
  - n â‰¥ 30 æ—¶è¿‘ä¼¼é€šå¸¸è¶³å¤Ÿå¥½
  - è§£é‡Šäº†ä¸ºä»€ä¹ˆæ­£æ€åˆ†å¸ƒæ— å¤„ä¸åœ¨
  - æ˜¯ç½®ä¿¡åŒºé—´å’Œå‡è®¾æ£€éªŒçš„åŸºç¡€
\`\`\`

### åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨

| æ¦‚å¿µ | DL ä¸­çš„ä½“ç° |
|------|------------|
| Gaussian | æƒé‡åˆå§‹åŒ– $W \\sim N(0, \\sigma^2)$ |
| Bernoulli | Dropout: æ¯ä¸ªç¥ç»å…ƒä»¥æ¦‚ç‡ p è¢«ä¿ç•™ |
| Categorical | Softmax è¾“å‡º = ç±»åˆ«æ¦‚ç‡åˆ†å¸ƒ |
| KL Divergence | VAE æ­£åˆ™é¡¹: $D_{KL}(q(z|x) \\| p(z))$ |
| Reparameterization | VAE é‡‡æ ·: $z = \\mu + \\sigma \\cdot \\epsilon, \\epsilon \\sim N(0,1)$ |
| Gumbel-Softmax | ç¦»æ•£é‡‡æ ·çš„å¯å¾®è¿‘ä¼¼ |`,
  },
  {
    id: 'cml9s5ebq000zt4xigcjts535',
    content: `## ä¼˜åŒ–å™¨æ¼”è¿› â€” ä» SGD åˆ° AdamW

ä¼˜åŒ–æ˜¯è®­ç»ƒæ‰€æœ‰ ML æ¨¡å‹çš„æ ¸å¿ƒå¼•æ“ã€‚é€‰æ‹©æ­£ç¡®çš„ä¼˜åŒ–å™¨å’Œè¶…å‚æ•°ç›´æ¥å†³å®šæ¨¡å‹èƒ½å¦æ”¶æ•›ã€æ”¶æ•›é€Ÿåº¦ä»¥åŠæœ€ç»ˆæ€§èƒ½ã€‚

### ä¼˜åŒ–é—®é¢˜çš„æœ¬è´¨

\`\`\`
Î¸* = argmin L(Î¸; D)

L: æŸå¤±å‡½æ•° (äº¤å‰ç†µã€MSEã€ç­‰)
Î¸: æ¨¡å‹å‚æ•° (å¯èƒ½æ•°åäº¿ç»´)
D: è®­ç»ƒæ•°æ®

æŒ‘æˆ˜:
- éå‡¸: æ·±åº¦ç½‘ç»œçš„æŸå¤±landscapeé«˜åº¦éå‡¸
- é«˜ç»´: GPT-3 æœ‰ 175B å‚æ•°
- å™ªå£°: åªç”¨ mini-batch ä¼°è®¡æ¢¯åº¦
- éç‚¹: é«˜ç»´ç©ºé—´éç‚¹è¿œå¤šäºå±€éƒ¨æœ€ä¼˜
\`\`\`

### Vanilla SGD
\`\`\`
Î¸_{t+1} = Î¸_t - Î· Â· âˆ‡L(Î¸_t; B_t)

Î·: å­¦ä¹ ç‡ (æœ€é‡è¦çš„è¶…å‚æ•°)
B_t: ç¬¬ t æ­¥çš„ mini-batch

é—®é¢˜:
1. å­¦ä¹ ç‡å›ºå®šï¼Œæ‰€æœ‰å‚æ•°å…±äº«åŒä¸€æ­¥é•¿
2. åœ¨éç‚¹é™„è¿‘æŒ¯è¡
3. å¯¹ç¨€ç–ç‰¹å¾ä¸å‹å¥½
4. å®¹æ˜“åœ¨ç‹­çª„å±±è°·ä¸­æ¥å›éœ‡è¡
\`\`\`

### Momentum (åŠ¨é‡)
\`\`\`
v_{t+1} = Î²Â·v_t + âˆ‡L(Î¸_t)         # ç§¯ç´¯åŠ¨é‡
Î¸_{t+1} = Î¸_t - Î·Â·v_{t+1}

Î² é€šå¸¸å– 0.9

ç‰©ç†ç›´è§‰: å°çƒåœ¨æŸå¤±å‡½æ•°æ›²é¢ä¸Šæ»šåŠ¨ï¼Œå¸¦æœ‰æƒ¯æ€§
\`\`\`

**Nesterov Momentum (NAG)**:
\`\`\`
v_{t+1} = Î²Â·v_t + âˆ‡L(Î¸_t - Î·Â·Î²Â·v_t)   # åœ¨"å‰ç»"ä½ç½®è®¡ç®—æ¢¯åº¦
Î¸_{t+1} = Î¸_t - Î·Â·v_{t+1}

ä¼˜åŠ¿: åœ¨æ¥è¿‘æœ€ä¼˜ç‚¹æ—¶èƒ½æ›´å¿«å‡é€Ÿï¼Œå‡å°‘æŒ¯è¡
\`\`\`

### AdaGrad (è‡ªé€‚åº”æ¢¯åº¦)
\`\`\`
g_t = âˆ‡L(Î¸_t)
G_t = G_{t-1} + g_tÂ²                   # ç´¯ç§¯æ¢¯åº¦å¹³æ–¹
Î¸_{t+1} = Î¸_t - Î· / (âˆšG_t + Îµ) Â· g_t

æ¯ä¸ªå‚æ•°æœ‰ç‹¬ç«‹çš„æœ‰æ•ˆå­¦ä¹ ç‡ï¼
é¢‘ç¹æ›´æ–°çš„å‚æ•° â†’ å­¦ä¹ ç‡å˜å°
ç¨€å°‘æ›´æ–°çš„å‚æ•° â†’ å­¦ä¹ ç‡ä¿æŒå¤§

é—®é¢˜: G_t å•è°ƒé€’å¢ â†’ å­¦ä¹ ç‡æœ€ç»ˆè¶‹è¿‘äº 0
\`\`\`

### RMSProp (è§£å†³ AdaGrad çš„è¡°å‡é—®é¢˜)
\`\`\`
v_t = Î²Â·v_{t-1} + (1-Î²)Â·g_tÂ²          # æŒ‡æ•°ç§»åŠ¨å¹³å‡
Î¸_{t+1} = Î¸_t - Î· / (âˆšv_t + Îµ) Â· g_t

Î² é€šå¸¸å– 0.9
ç”¨æŒ‡æ•°è¡°å‡æ›¿ä»£ç´¯ç§¯ â†’ å­¦ä¹ ç‡ä¸ä¼šè¶‹äº 0
\`\`\`

### Adam (Adaptive Moment Estimation)
\`\`\`
= Momentum + RMSProp

m_t = Î²â‚Â·m_{t-1} + (1-Î²â‚)Â·g_t        # ä¸€é˜¶çŸ© (æ¢¯åº¦å‡å€¼)
v_t = Î²â‚‚Â·v_{t-1} + (1-Î²â‚‚)Â·g_tÂ²       # äºŒé˜¶çŸ© (æ¢¯åº¦æ–¹å·®)

# åå·®ä¿®æ­£ (åˆå§‹é˜¶æ®µ m, v åå‘ 0)
mÌ‚_t = m_t / (1-Î²â‚áµ—)
vÌ‚_t = v_t / (1-Î²â‚‚áµ—)

Î¸_{t+1} = Î¸_t - Î· Â· mÌ‚_t / (âˆšvÌ‚_t + Îµ)

é»˜è®¤è¶…å‚æ•°:
  Î²â‚ = 0.9, Î²â‚‚ = 0.999, Îµ = 1e-8, Î· = 1e-3 (æˆ– 3e-4)
\`\`\`

### AdamW (Weight Decay ä¿®æ­£) â€” å½“å‰æ ‡å‡†

\`\`\`
åŸå§‹ Adam çš„ L2 æ­£åˆ™åŒ–:
  g_t = âˆ‡L(Î¸_t) + Î»Â·Î¸_t              # é”™è¯¯: decay è¢«è‡ªé€‚åº”å­¦ä¹ ç‡ç¼©æ”¾

AdamW çš„è§£è€¦ Weight Decay:
  Î¸_{t+1} = Î¸_t - Î· Â· (mÌ‚_t / (âˆšvÌ‚_t + Îµ) + Î»Â·Î¸_t)

åŒºåˆ«: weight decay ä¸ç»è¿‡è‡ªé€‚åº”è°ƒæ•´
æ•ˆæœ: æ­£åˆ™åŒ–æ›´å‡åŒ€ï¼Œæ³›åŒ–æ›´å¥½

AdamW æ˜¯å½“å‰ Transformer / LLM è®­ç»ƒçš„æ ‡å‡†ä¼˜åŒ–å™¨
\`\`\`

### ä¼˜åŒ–å™¨é€‰æ‹©æŒ‡å—

| åœºæ™¯ | æ¨èä¼˜åŒ–å™¨ | å­¦ä¹ ç‡ |
|------|-----------|--------|
| LLM é¢„è®­ç»ƒ | AdamW | 1e-4 ~ 3e-4 |
| LLM å¾®è°ƒ | AdamW | 1e-5 ~ 5e-5 |
| CNN è®­ç»ƒ | SGD + Momentum | 0.01 ~ 0.1 |
| GAN è®­ç»ƒ | Adam (Î²â‚=0, Î²â‚‚=0.9) | 1e-4 ~ 2e-4 |
| å°æ¨¡å‹å¿«é€Ÿå®éªŒ | Adam | 1e-3 |

### å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥

| ç­–ç•¥ | å…¬å¼ | é€‚ç”¨åœºæ™¯ |
|------|------|---------|
| Step Decay | $\\eta = \\eta_0 \\cdot \\gamma^{\\lfloor epoch/step \\rfloor}$ | ä¼ ç»Ÿ CNN |
| Cosine Annealing | $\\eta = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})(1 + \\cos(\\frac{\\pi t}{T}))$ | Transformer |
| Warmup + Cosine | çº¿æ€§å¢é•¿ â†’ cosine è¡°å‡ | LLM é¢„è®­ç»ƒ |
| OneCycleLR | warmup â†’ max â†’ è¡°å‡ | å¿«é€Ÿæ”¶æ•› |
| Inverse Sqrt | $\\eta = \\eta_0 / \\sqrt{t}$ | åŸå§‹ Transformer |

\`\`\`
Warmup çš„å¿…è¦æ€§:
- è®­ç»ƒåˆæœŸæ¢¯åº¦ä¸ç¨³å®šï¼ˆéšæœºåˆå§‹åŒ–çš„æƒé‡ï¼‰
- ç›´æ¥ç”¨å¤§å­¦ä¹ ç‡å®¹æ˜“å‘æ•£
- é€šå¸¸ warmup 1000-4000 æ­¥
- LLaMA 65B: 2000 æ­¥ warmup + cosine decay
\`\`\`

### å‡¸ä¼˜åŒ– vs éå‡¸ä¼˜åŒ–

\`\`\`
å‡¸ä¼˜åŒ–:
- å±€éƒ¨æœ€ä¼˜ = å…¨å±€æœ€ä¼˜
- ä¿è¯æ”¶æ•›åˆ°æœ€ä¼˜è§£
- SVMã€çº¿æ€§å›å½’ã€é€»è¾‘å›å½’

éå‡¸ä¼˜åŒ– (æ·±åº¦å­¦ä¹ ):
- å¤§é‡å±€éƒ¨æœ€ä¼˜å’Œéç‚¹
- ä½†å®è¯è¡¨æ˜: ä¸åŒå±€éƒ¨æœ€ä¼˜æ€§èƒ½ç›¸è¿‘
- çœŸæ­£çš„é—®é¢˜æ˜¯éç‚¹å’Œå¹³å¦åŒºåŸŸ
- é«˜ç»´ç©ºé—´: éç‚¹æ¯”å±€éƒ¨æœ€ä¼˜å¤šå¾—å¤š
\`\`\`

### æ¢¯åº¦é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

| é—®é¢˜ | åŸå›  | è§£å†³æ–¹æ¡ˆ |
|------|------|---------|
| æ¢¯åº¦æ¶ˆå¤± | sigmoid/tanh é¥±å’ŒåŒº | ReLU, æ®‹å·®è¿æ¥, LayerNorm |
| æ¢¯åº¦çˆ†ç‚¸ | æ·±å±‚ç½‘ç»œç´¯ç§¯ä¹˜æ³• | æ¢¯åº¦è£å‰ª, åˆé€‚çš„åˆå§‹åŒ– |
| ç¨€ç–æ¢¯åº¦ | åµŒå…¥å±‚ç­‰ç¦»æ•£ç‰¹å¾ | AdaGrad ç³»åˆ— |
| å™ªå£°å¤§ | batch å¤ªå° | å¢å¤§ batch, æ¢¯åº¦ç´¯ç§¯ |

\`\`\`
æ¢¯åº¦è£å‰ª (Gradient Clipping):
  æ–¹å¼1 - æŒ‰èŒƒæ•°: if ||g|| > threshold: g = g Â· threshold / ||g||
  æ–¹å¼2 - æŒ‰å€¼: g = clip(g, -threshold, threshold)

  LLM å¸¸ç”¨: max_grad_norm = 1.0
\`\`\``,
  },
  {
    id: 'cml9s5ebr0014t4xiwhda96pi',
    content: `## ä¿¡æ¯è®ºä¸ ML çš„æ¡¥æ¢

ä¿¡æ¯è®ºç”± Claude Shannon äº 1948 å¹´åˆ›ç«‹ï¼ŒåŸæœ¬ç”¨äºé€šä¿¡ç³»ç»Ÿã€‚ç„¶è€Œï¼Œä¿¡æ¯è®ºçš„æ ¸å¿ƒæ¦‚å¿µï¼ˆç†µã€äº’ä¿¡æ¯ã€KL æ•£åº¦ï¼‰å·²æˆä¸ºç°ä»£ ML çš„ç†è®ºåŸºçŸ³ï¼Œä»æŸå¤±å‡½æ•°è®¾è®¡åˆ°ç”Ÿæˆæ¨¡å‹ã€ä»ç‰¹å¾é€‰æ‹©åˆ°æ·±åº¦å­¦ä¹ çš„æ³›åŒ–ç†è®ºï¼Œæ— å¤„ä¸åœ¨ã€‚

### æ ¸å¿ƒæ¦‚å¿µè¯¦è§£

**ä¿¡æ¯é‡ (Self-Information)**
\`\`\`
I(x) = -logâ‚‚ P(x)    (å•ä½: bit)
I(x) = -ln P(x)       (å•ä½: nat)

ç›´è§‰: è¶Šä¸å¯èƒ½çš„äº‹ä»¶ï¼ŒåŒ…å«çš„ä¿¡æ¯é‡è¶Šå¤§
ä¾‹: "å¤ªé˜³ä»ä¸œæ–¹å‡èµ·" â†’ ä¿¡æ¯é‡ â‰ˆ 0
    "æ˜å¤©æœ‰æµæ˜Ÿé›¨" â†’ ä¿¡æ¯é‡å¤§
\`\`\`

**ç†µ (Entropy) â€” ä¸ç¡®å®šæ€§çš„åº¦é‡**

$$H(X) = -\\sum_{x} P(x) \\log P(x) = \\mathbb{E}[-\\log P(X)]$$

\`\`\`
æ€§è´¨:
- H(X) â‰¥ 0ï¼Œå½“ä¸”ä»…å½“ X ç¡®å®šæ—¶ H(X) = 0
- å‡åŒ€åˆ†å¸ƒæ—¶ç†µæœ€å¤§: H = log(n)
- äºŒå…ƒç†µ: H(p) = -p log(p) - (1-p) log(1-p)
  p=0.5 æ—¶æœ€å¤§ (= 1 bit)

ä¾‹: å…¬å¹³ç¡¬å¸ H = 1 bit
    åå‘ç¡¬å¸ (p=0.9) H = 0.47 bit
    ç¡®å®šç»“æœ (p=1.0) H = 0 bit
\`\`\`

**æ¡ä»¶ç†µ (Conditional Entropy)**

$$H(Y|X) = -\\sum_{x,y} P(x,y) \\log P(y|x)$$

\`\`\`
å«ä¹‰: å·²çŸ¥ X åï¼ŒY çš„å‰©ä½™ä¸ç¡®å®šæ€§
æ€§è´¨: H(Y|X) â‰¤ H(Y)ï¼Œç­‰å·å½“ä¸”ä»…å½“ X, Y ç‹¬ç«‹
\`\`\`

**è”åˆç†µ (Joint Entropy)**

$$H(X, Y) = H(X) + H(Y|X) = H(Y) + H(X|Y)$$

### äº¤å‰ç†µ (Cross-Entropy) â€” ML æœ€å¸¸ç”¨çš„æŸå¤±å‡½æ•°

$$H(p, q) = -\\sum_{x} p(x) \\log q(x) = \\mathbb{E}_p[-\\log q(X)]$$

\`\`\`
å«ä¹‰: ç”¨åˆ†å¸ƒ q ç¼–ç æ¥è‡ªåˆ†å¸ƒ p çš„æ•°æ®ï¼Œå¹³å‡éœ€è¦çš„ bit æ•°

åœ¨åˆ†ç±»ä»»åŠ¡ä¸­:
  p = çœŸå®æ ‡ç­¾çš„ one-hot åˆ†å¸ƒ
  q = æ¨¡å‹é¢„æµ‹çš„æ¦‚ç‡åˆ†å¸ƒ

äºŒåˆ†ç±»äº¤å‰ç†µ:
  L = -[yÂ·log(Å·) + (1-y)Â·log(1-Å·)]

å¤šåˆ†ç±»äº¤å‰ç†µ:
  L = -Î£ y_i Â· log(Å·_i)
  å…¶ä¸­ Å· = softmax(logits)

ä¸ºä»€ä¹ˆç”¨äº¤å‰ç†µè€Œä¸æ˜¯ MSEï¼Ÿ
  1. æ¢¯åº¦æ›´å¥½: äº¤å‰ç†µçš„æ¢¯åº¦ä¸ä¼šåœ¨é¥±å’ŒåŒºæ¶ˆå¤±
  2. æ¦‚ç‡è§£é‡Š: æœ€å°åŒ–äº¤å‰ç†µ = æœ€å¤§åŒ–ä¼¼ç„¶
  3. ä¿¡æ¯è®ºæ„ä¹‰: æ‰¾åˆ°æœ€æ¥è¿‘çœŸå®åˆ†å¸ƒçš„æ¨¡å‹
\`\`\`

### KL æ•£åº¦ (Kullback-Leibler Divergence) â€” åˆ†å¸ƒå·®å¼‚åº¦é‡

$$D_{KL}(p \\| q) = \\sum_{x} p(x) \\log \\frac{p(x)}{q(x)} = H(p, q) - H(p)$$

\`\`\`
æ€§è´¨:
- D_KL â‰¥ 0 (Gibbs ä¸ç­‰å¼)
- D_KL(p||q) = 0 å½“ä¸”ä»…å½“ p = q
- ä¸å¯¹ç§°: D_KL(p||q) â‰  D_KL(q||p) â†’ ä¸æ˜¯è·ç¦»ï¼

ä¸¤ç§æ–¹å‘çš„åŒºåˆ«:
  D_KL(p||q) - "å‰å‘ KL": q åœ¨ p éé›¶å¤„å¿…é¡»éé›¶ â†’ å‡å€¼å¯»æ‰¾
  D_KL(q||p) - "åå‘ KL": q å€¾å‘äºé›†ä¸­åœ¨ p çš„æŸä¸ªæ¨¡å¼ â†’ æ¨¡å¼å¯»æ‰¾

  VAE ç”¨å‰å‘ KL â†’ ç”Ÿæˆå¤šæ ·ä½†å¯èƒ½æ¨¡ç³Š
  GAN éšå¼ç”¨åå‘ KL â†’ ç”Ÿæˆæ¸…æ™°ä½†å¯èƒ½ç¼ºå°‘å¤šæ ·æ€§
\`\`\`

**å…³é”®å…³ç³»:**
$$H(p, q) = H(p) + D_{KL}(p \\| q)$$

æœ€å°åŒ–äº¤å‰ç†µ $H(p,q)$ = æœ€å°åŒ– $D_{KL}(p \\| q)$ï¼ˆå› ä¸º $H(p)$ æ˜¯å¸¸æ•°ï¼‰

### äº’ä¿¡æ¯ (Mutual Information)

$$I(X; Y) = H(X) - H(X|Y) = H(Y) - H(Y|X) = D_{KL}(P(X,Y) \\| P(X)P(Y))$$

\`\`\`
å«ä¹‰: X å’Œ Y å…±äº«çš„ä¿¡æ¯é‡
     = çŸ¥é“ Y åï¼ŒX ä¸ç¡®å®šæ€§å‡å°‘äº†å¤šå°‘

æ€§è´¨:
- I(X;Y) â‰¥ 0
- I(X;Y) = 0 å½“ä¸”ä»…å½“ X, Y ç‹¬ç«‹
- I(X;Y) = I(Y;X) (å¯¹ç§°)
- I(X;X) = H(X) (è‡ªäº’ä¿¡æ¯ = ç†µ)
\`\`\`

### åœ¨ ML å„é¢†åŸŸä¸­çš„åº”ç”¨

| æ¦‚å¿µ | ML åº”ç”¨ | å…·ä½“å®ä¾‹ |
|------|---------|---------|
| äº¤å‰ç†µ | åˆ†ç±»æŸå¤±å‡½æ•° | Softmax + CE = å‡ ä¹æ‰€æœ‰åˆ†ç±»å™¨ |
| KL æ•£åº¦ | VAE æ­£åˆ™é¡¹ | $D_{KL}(q(z|x) \\| p(z))$ çº¦æŸæ½œç©ºé—´ |
| KL æ•£åº¦ | çŸ¥è¯†è’¸é¦ | å­¦ç”Ÿæ¨¡å‹æ¨¡ä»¿æ•™å¸ˆçš„è½¯æ ‡ç­¾åˆ†å¸ƒ |
| KL æ•£åº¦ | RLHF ä¸­çš„ KL æƒ©ç½š | é˜²æ­¢ç­–ç•¥åç¦»å‚è€ƒæ¨¡å‹å¤ªè¿œ |
| äº’ä¿¡æ¯ | å¯¹æ¯”å­¦ä¹  (InfoNCE) | æœ€å¤§åŒ–æ­£æ ·æœ¬å¯¹çš„äº’ä¿¡æ¯ |
| äº’ä¿¡æ¯ | ç‰¹å¾é€‰æ‹© | é€‰æ‹©ä¸ç›®æ ‡äº’ä¿¡æ¯æœ€å¤§çš„ç‰¹å¾ |
| æ¡ä»¶ç†µ | å†³ç­–æ ‘åˆ†è£‚ | ä¿¡æ¯å¢ç›Š = H(Y) - H(Y|X) |
| æœ€å¤§ç†µ | Softmax ç†è®ºåŸºç¡€ | åœ¨çº¦æŸä¸‹ç†µæœ€å¤§çš„åˆ†å¸ƒ |
| ä¿¡æ¯ç“¶é¢ˆ | DL æ³›åŒ–ç†è®º | ä¸­é—´å±‚å‹ç¼©è¾“å…¥ä¿¡æ¯ã€ä¿ç•™ç›®æ ‡ä¿¡æ¯ |
| æ•°æ®å‹ç¼© | å˜é•¿ç¼–ç  | Huffman ç¼–ç è¾¾åˆ°ç†µçš„ä¸‹ç•Œ |

### ä¿¡æ¯ç“¶é¢ˆç†è®º (Information Bottleneck)

\`\`\`
Tishby (2015) æå‡ºçš„æ·±åº¦å­¦ä¹ ç†è®ºæ¡†æ¶:

ç›®æ ‡: max I(T; Y) - Î² Â· I(T; X)
  T: ä¸­é—´å±‚è¡¨ç¤º
  X: è¾“å…¥
  Y: ç›®æ ‡

å«ä¹‰: ä¸­é—´å±‚åº”è¯¥:
  1. ä¿ç•™å…³äº Y çš„ä¿¡æ¯ (I(T;Y) å¤§)
  2. å‹ç¼©å…³äº X çš„å†—ä½™ä¿¡æ¯ (I(T;X) å°)

è®­ç»ƒè¿‡ç¨‹çš„ä¸¤ä¸ªé˜¶æ®µ:
  Phase 1 (æ‹Ÿåˆ): I(T;Y) å¿«é€Ÿå¢åŠ 
  Phase 2 (å‹ç¼©): I(T;X) é€æ¸å‡å° â†’ æ³›åŒ–ï¼

äº‰è®®: è¿™ä¸ªç†è®ºæ˜¯å¦æ™®éé€‚ç”¨ä»æœ‰è®¨è®º
\`\`\`

### Jensen-Shannon Divergence (JSD)

\`\`\`
JSD(p||q) = 0.5 Â· D_KL(p||m) + 0.5 Â· D_KL(q||m)
å…¶ä¸­ m = 0.5 Â· (p + q)

ä¼˜åŠ¿:
- å¯¹ç§°: JSD(p||q) = JSD(q||p)
- æœ‰ç•Œ: 0 â‰¤ JSD â‰¤ 1 (ä»¥ logâ‚‚ ä¸ºåº•)
- æ€»æ˜¯æœ‰é™çš„ï¼ˆå³ä½¿æ”¯æ’‘é›†ä¸åŒï¼‰

åº”ç”¨: åŸå§‹ GAN çš„è®­ç»ƒç›®æ ‡æœ¬è´¨ä¸Šåœ¨æœ€å°åŒ– JSD
\`\`\``,
  },
]

const NEW_NOTES = [
  {
    taskTitle: 'Linear Algebra for AI',
    title: 'å‘é‡ç©ºé—´ä¸çº¿æ€§å˜æ¢',
    content: `## å‘é‡ç©ºé—´ä¸çº¿æ€§å˜æ¢ â€” AI çš„å‡ ä½•ç›´è§‰

### å‘é‡ç©ºé—´çš„å…¬ç†å®šä¹‰

ä¸€ä¸ªå‘é‡ç©ºé—´ $(V, F, +, \\cdot)$ æ»¡è¶³ 8 æ¡å…¬ç†ï¼ˆåŠ æ³• 4 æ¡ + æ•°ä¹˜ 4 æ¡ï¼‰ã€‚ä½†æ›´é‡è¦çš„æ˜¯**ç›´è§‰**ï¼š

\`\`\`
å‘é‡ç©ºé—´ = ä¸€ä¸ªå¯ä»¥åšçº¿æ€§ç»„åˆçš„é›†åˆ

çº¿æ€§ç»„åˆ: câ‚vâ‚ + câ‚‚vâ‚‚ + ... + câ‚™vâ‚™
å¼ æˆç©ºé—´: span{vâ‚, ..., vâ‚™} = æ‰€æœ‰çº¿æ€§ç»„åˆçš„é›†åˆ
åŸº:        çº¿æ€§æ— å…³çš„å¼ æˆé›†
ç»´åº¦:      åŸºå‘é‡çš„ä¸ªæ•°
\`\`\`

### å­ç©ºé—´çš„ ML æ„ä¹‰

| å­ç©ºé—´ | å®šä¹‰ | ML åº”ç”¨ |
|--------|------|---------|
| åˆ—ç©ºé—´ Col(A) | A çš„åˆ—å‘é‡å¼ æˆçš„ç©ºé—´ | çº¿æ€§æ¨¡å‹çš„å¯è¾¾è¾“å‡ºç©ºé—´ |
| é›¶ç©ºé—´ Null(A) | Ax=0 çš„è§£é›† | æ­£åˆ™åŒ–/çº¦æŸçš„è‡ªç”±åº¦ |
| è¡Œç©ºé—´ Row(A) | A çš„è¡Œå‘é‡å¼ æˆçš„ç©ºé—´ | æ•°æ®çš„æœ‰æ•ˆç»´åº¦ |
| å·¦é›¶ç©ºé—´ | A^T y=0 çš„è§£é›† | ä¸å¯è¾¾çš„è¾“å‡ºæ–¹å‘ |

### çº¿æ€§å˜æ¢

\`\`\`
å®šä¹‰: T: V â†’ W æ»¡è¶³
  T(u + v) = T(u) + T(v)
  T(cv) = cT(v)

å…³é”®: æ¯ä¸ªçº¿æ€§å˜æ¢éƒ½å¯ä»¥ç”¨çŸ©é˜µè¡¨ç¤º!
  [T(v)]_B' = A Â· [v]_B

å…¶ä¸­ A çš„åˆ— = åŸºå‘é‡åœ¨ T ä¸‹çš„åƒ
\`\`\`

### ç‰¹å¾å€¼åˆ†è§£çš„å‡ ä½•ç›´è§‰

\`\`\`
Av = Î»v

å«ä¹‰: å¯¹äºç‰¹å¾å‘é‡ vï¼ŒçŸ©é˜µ A çš„ä½œç”¨åªæ˜¯ç¼©æ”¾ï¼ˆÎ» å€ï¼‰
  Î» > 1: æ‹‰ä¼¸
  0 < Î» < 1: å‹ç¼©
  Î» < 0: ç¿»è½¬ + ç¼©æ”¾
  Î» = 0: æŠ•å½±åˆ°é›¶ï¼ˆä¿¡æ¯ä¸¢å¤±ï¼‰

åœ¨ PCA ä¸­:
  åæ–¹å·®çŸ©é˜µçš„ç‰¹å¾å€¼ = å„ä¸»æˆåˆ†æ–¹å‘çš„æ–¹å·®
  ç‰¹å¾å‘é‡ = ä¸»æˆåˆ†æ–¹å‘
  å¤§ç‰¹å¾å€¼ â†’ æ•°æ®åœ¨è¯¥æ–¹å‘ä¸Šå˜åŒ–å¤š â†’ é‡è¦
  å°ç‰¹å¾å€¼ â†’ æ•°æ®åœ¨è¯¥æ–¹å‘ä¸Šå˜åŒ–å°‘ â†’ å¯ä¸¢å¼ƒ
\`\`\`

### åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨

**1. åµŒå…¥ç©ºé—´æ˜¯å‘é‡ç©ºé—´**
\`\`\`
Word2Vec çš„ç»å…¸ä¾‹å­:
  vec("King") - vec("Man") + vec("Woman") â‰ˆ vec("Queen")

è¯´æ˜åµŒå…¥ç©ºé—´ä¸­å­˜åœ¨æœ‰æ„ä¹‰çš„çº¿æ€§ç»“æ„
\`\`\`

**2. æƒé‡çŸ©é˜µ = çº¿æ€§å˜æ¢**
\`\`\`
y = Wx + b
W å°†è¾“å…¥ç©ºé—´æ˜ å°„åˆ°è¾“å‡ºç©ºé—´
ä¸åŒå±‚çš„ W å­¦åˆ°ä¸åŒçš„"è§†è§’"
\`\`\`

**3. æ³¨æ„åŠ›çŸ©é˜µçš„ç§©**
\`\`\`
ä½ç§©æ³¨æ„åŠ› â†’ æ¨¡å‹åœ¨åšç®€å•çš„"æŸ¥æ‰¾"
é«˜ç§©æ³¨æ„åŠ› â†’ æ¨¡å‹åœ¨åšå¤æ‚çš„è·¨ä½ç½®æ¨ç†
è¿™æ˜¯ LoRA æœ‰æ•ˆçš„ç†è®ºåŸºç¡€ä¹‹ä¸€
\`\`\``,
  },
  {
    taskTitle: 'Probability & Statistics',
    title: 'è´å¶æ–¯æ¨æ–­ä¸ MCMC æ–¹æ³•',
    content: `## è´å¶æ–¯æ¨æ–­æ¡†æ¶

### è´å¶æ–¯å®šç†

$$P(\\theta | D) = \\frac{P(D | \\theta) \\cdot P(\\theta)}{P(D)}$$

\`\`\`
P(Î¸|D):   åéªŒ â€” çœ‹åˆ°æ•°æ®åå¯¹å‚æ•°çš„ä¿¡å¿µ
P(D|Î¸):   ä¼¼ç„¶ â€” ç»™å®šå‚æ•°ï¼Œæ•°æ®çš„æ¦‚ç‡
P(Î¸):     å…ˆéªŒ â€” çœ‹åˆ°æ•°æ®å‰å¯¹å‚æ•°çš„ä¿¡å¿µ
P(D):     è¯æ® â€” å½’ä¸€åŒ–å¸¸æ•° = âˆ«P(D|Î¸)P(Î¸)dÎ¸

è´å¶æ–¯çš„æ ¸å¿ƒæ€æƒ³: å…ˆéªŒ + æ•°æ® â†’ åéªŒ
\`\`\`

### ä¸ºä»€ä¹ˆ P(D) éš¾ä»¥è®¡ç®—

\`\`\`
P(D) = âˆ« P(D|Î¸) Â· P(Î¸) dÎ¸

è¿™æ˜¯ä¸€ä¸ªé«˜ç»´ç§¯åˆ†:
- å¦‚æœ Î¸ æœ‰ 1000 ä¸ªç»´åº¦
- æ•°å€¼ç§¯åˆ†åœ¨é«˜ç»´ç©ºé—´ä¸­è®¡ç®—é‡æŒ‡æ•°çˆ†ç‚¸
- è¿™å°±æ˜¯ä¸ºä»€ä¹ˆéœ€è¦ MCMC æˆ–å˜åˆ†æ¨æ–­

è§£å†³æ–¹æ¡ˆ:
1. å…±è½­å…ˆéªŒ: æœ‰è§£æè§£ï¼Œä¸éœ€è¦ç®— P(D)
2. MCMC: ä»åéªŒä¸­é‡‡æ ·ï¼Œä¸éœ€è¦çŸ¥é“ P(D)
3. å˜åˆ†æ¨æ–­: ç”¨ä¼˜åŒ–ä»£æ›¿é‡‡æ ·
\`\`\`

### MCMC (é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡æ´›)

**æ ¸å¿ƒæ€æƒ³**: æ„é€ ä¸€ä¸ªé©¬å°”å¯å¤«é“¾ï¼Œä½¿å…¶å¹³ç¨³åˆ†å¸ƒ = ç›®æ ‡åéªŒåˆ†å¸ƒ

**Metropolis-Hastings ç®—æ³•**:
\`\`\`
1. åˆå§‹åŒ– Î¸â‚€
2. for t = 1, 2, ..., T:
   a. æè®®: Î¸* ~ q(Î¸*|Î¸_t)        # ä»æè®®åˆ†å¸ƒé‡‡æ ·
   b. æ¥å—æ¦‚ç‡:
      Î± = min(1, P(Î¸*|D)Â·q(Î¸_t|Î¸*) / P(Î¸_t|D)Â·q(Î¸*|Î¸_t))
      æ³¨æ„: P(D) åœ¨åˆ†å­åˆ†æ¯ä¸­æ¶ˆæ‰äº†ï¼
   c. ä»¥æ¦‚ç‡ Î± æ¥å— Î¸_{t+1} = Î¸*
      å¦åˆ™ Î¸_{t+1} = Î¸_t

3. ä¸¢å¼ƒå‰ B ä¸ªæ ·æœ¬ (burn-in)
4. å‰©ä½™æ ·æœ¬è¿‘ä¼¼åéªŒåˆ†å¸ƒ
\`\`\`

**Gibbs Sampling**: MH çš„ç‰¹ä¾‹ï¼Œæ¯æ¬¡åªæ›´æ–°ä¸€ä¸ªç»´åº¦
\`\`\`
for each dimension i:
  Î¸_i ~ P(Î¸_i | Î¸_{-i}, D)    # ä»æ¡ä»¶åˆ†å¸ƒé‡‡æ ·

ä¼˜åŠ¿: ä¸éœ€è¦è°ƒèŠ‚æè®®åˆ†å¸ƒï¼Œæ¥å—ç‡ 100%
æ¡ä»¶: éœ€è¦æ¡ä»¶åˆ†å¸ƒå¯ä»¥é‡‡æ ·
åº”ç”¨: LDA ä¸»é¢˜æ¨¡å‹çš„ç»å…¸æ¨æ–­æ–¹æ³•
\`\`\`

**HMC (Hamiltonian Monte Carlo)**: åˆ©ç”¨æ¢¯åº¦ä¿¡æ¯çš„ MCMC
\`\`\`
å°†å‚æ•°ç©ºé—´ç±»æ¯”ä¸ºç‰©ç†ç³»ç»Ÿ:
  Î¸ = ä½ç½®, p = åŠ¨é‡
  H(Î¸, p) = U(Î¸) + K(p) = -log P(Î¸|D) + pÂ²/2

æ¨¡æ‹Ÿå“ˆå¯†é¡¿åŠ›å­¦è½¨è¿¹ â†’ æ›´é«˜æ•ˆçš„é‡‡æ ·
åœ¨é«˜ç»´ç©ºé—´ä¸­è¿œä¼˜äºéšæœºæ¸¸èµ° MH

NUTS (No-U-Turn Sampler):
  è‡ªåŠ¨è°ƒèŠ‚ HMC çš„æ­¥é•¿å’Œæ­¥æ•°
  PyMCã€Stan çš„é»˜è®¤ç®—æ³•
\`\`\`

### å˜åˆ†æ¨æ–­ (Variational Inference)

\`\`\`
å°†æ¨æ–­è½¬åŒ–ä¸ºä¼˜åŒ–é—®é¢˜:

ç›®æ ‡: æ‰¾ä¸€ä¸ªç®€å•åˆ†å¸ƒ q(Î¸) â‰ˆ P(Î¸|D)
æ–¹æ³•: æœ€å°åŒ– KL(q(Î¸) || P(Î¸|D))
ç­‰ä»·: æœ€å¤§åŒ– ELBO (Evidence Lower BOund)

ELBO = E_q[log P(D,Î¸)] - E_q[log q(Î¸)]
     = E_q[log P(D|Î¸)] - KL(q(Î¸) || P(Î¸))

Mean-Field VI: å‡è®¾ q(Î¸) = Î  q_i(Î¸_i)

MCMC vs VI:
  MCMC: ç²¾ç¡®(æ¸è¿‘)ï¼Œæ…¢ï¼Œé€‚åˆå°æ•°æ®
  VI:   è¿‘ä¼¼ï¼Œå¿«ï¼Œå¯æ‰©å±•åˆ°å¤§æ•°æ®
\`\`\`

### è´å¶æ–¯æ·±åº¦å­¦ä¹ 

\`\`\`
æ ‡å‡† DL: ç‚¹ä¼°è®¡æƒé‡ W*
è´å¶æ–¯ DL: æƒé‡çš„åéªŒåˆ†å¸ƒ P(W|D)

æ–¹æ³•:
1. MC Dropout: æµ‹è¯•æ—¶ä¿ç•™ Dropout â†’ è¿‘ä¼¼è´å¶æ–¯
2. æ·±åº¦é›†æˆ: è®­ç»ƒå¤šä¸ªæ¨¡å‹ â†’ ç”¨æ–¹å·®ä¼°è®¡ä¸ç¡®å®šæ€§
3. è´å¶æ–¯ç¥ç»ç½‘ç»œ: æƒé‡ç”¨åˆ†å¸ƒè¡¨ç¤º
4. Laplace è¿‘ä¼¼: åœ¨ MAP ç‚¹åšäºŒé˜¶è¿‘ä¼¼

ä¸»è¦ä»·å€¼: ä¸ç¡®å®šæ€§ä¼°è®¡
  è®¤çŸ¥ä¸ç¡®å®šæ€§ (Epistemic): æ•°æ®ä¸è¶³å¯¼è‡´ â†’ å¯ä»¥é€šè¿‡æ›´å¤šæ•°æ®å‡å°‘
  å¶ç„¶ä¸ç¡®å®šæ€§ (Aleatoric): æ•°æ®æœ¬èº«çš„å™ªå£° â†’ ä¸å¯å‡å°‘
\`\`\``,
  },
  {
    taskTitle: 'Optimization Theory',
    title: 'çº¦æŸä¼˜åŒ–ä¸ KKT æ¡ä»¶',
    content: `## çº¦æŸä¼˜åŒ– â€” SVM å’Œæ­£åˆ™åŒ–çš„ç†è®ºåŸºç¡€

### æ— çº¦æŸ vs æœ‰çº¦æŸä¼˜åŒ–

\`\`\`
æ— çº¦æŸ: min f(x)
  è§£æ³•: âˆ‡f(x*) = 0

ç­‰å¼çº¦æŸ: min f(x) s.t. h(x) = 0
  è§£æ³•: æ‹‰æ ¼æœ—æ—¥ä¹˜å­æ³•

ä¸ç­‰å¼çº¦æŸ: min f(x) s.t. g(x) â‰¤ 0
  è§£æ³•: KKT æ¡ä»¶
\`\`\`

### æ‹‰æ ¼æœ—æ—¥ä¹˜å­æ³•

\`\`\`
é—®é¢˜: min f(x) s.t. h_i(x) = 0, i=1,...,m

æ‹‰æ ¼æœ—æ—¥å‡½æ•°:
  L(x, Î») = f(x) + Î£ Î»_i Â· h_i(x)

æœ€ä¼˜æ¡ä»¶:
  âˆ‡_x L = 0    (æ¢¯åº¦ä¸ºé›¶)
  h_i(x) = 0   (çº¦æŸæ»¡è¶³)

å‡ ä½•ç›´è§‰:
  åœ¨æœ€ä¼˜ç‚¹ï¼Œf çš„æ¢¯åº¦ âˆ h çš„æ¢¯åº¦
  å³: ç›®æ ‡å‡½æ•°çš„ç­‰é«˜çº¿ä¸çº¦æŸæ›²é¢ç›¸åˆ‡
\`\`\`

### KKT æ¡ä»¶ (Karush-Kuhn-Tucker)

\`\`\`
é—®é¢˜: min f(x)
      s.t. g_j(x) â‰¤ 0, j=1,...,p
           h_i(x) = 0, i=1,...,m

æ‹‰æ ¼æœ—æ—¥:
  L(x, Î», Î¼) = f(x) + Î£ Î»_i h_i(x) + Î£ Î¼_j g_j(x)

KKT å¿…è¦æ¡ä»¶:
  1. æ¢¯åº¦æ¡ä»¶:    âˆ‡_x L = 0
  2. åŸå§‹å¯è¡Œ:    g_j(x) â‰¤ 0, h_i(x) = 0
  3. å¯¹å¶å¯è¡Œ:    Î¼_j â‰¥ 0
  4. äº’è¡¥æ¾å¼›:    Î¼_j Â· g_j(x) = 0

äº’è¡¥æ¾å¼›çš„å«ä¹‰:
  Î¼_j > 0 â†’ g_j(x) = 0 (çº¦æŸæ¿€æ´»ï¼Œåœ¨è¾¹ç•Œä¸Š)
  g_j(x) < 0 â†’ Î¼_j = 0 (çº¦æŸä¸æ¿€æ´»ï¼Œåœ¨å†…éƒ¨)
\`\`\`

### SVM çš„å¯¹å¶æ¨å¯¼

\`\`\`
åŸå§‹é—®é¢˜:
  min  0.5Â·||w||Â²
  s.t. y_i(wÂ·x_i + b) â‰¥ 1

ç­‰ä»·:
  min  0.5Â·||w||Â²
  s.t. 1 - y_i(wÂ·x_i + b) â‰¤ 0

æ‹‰æ ¼æœ—æ—¥:
  L = 0.5||w||Â² - Î£ Î±_i [y_i(wÂ·x_i + b) - 1]

å¯¹ w, b æ±‚å¯¼ä»¤å…¶ä¸ºé›¶:
  âˆ‚L/âˆ‚w = 0 â†’ w = Î£ Î±_i y_i x_i
  âˆ‚L/âˆ‚b = 0 â†’ Î£ Î±_i y_i = 0

ä»£å…¥å¾—å¯¹å¶é—®é¢˜:
  max  Î£ Î±_i - 0.5 Î£Î£ Î±_i Î±_j y_i y_j (x_i Â· x_j)
  s.t. Î±_i â‰¥ 0, Î£ Î±_i y_i = 0

æ ¸æŠ€å·§: å°† x_i Â· x_j æ›¿æ¢ä¸º K(x_i, x_j)
  â†’ éšå¼åœ¨é«˜ç»´ç©ºé—´ä¸­åšçº¿æ€§åˆ†ç±»
\`\`\`

### å¯¹å¶æ€§ (Duality)

\`\`\`
åŸå§‹é—®é¢˜å€¼: p*
å¯¹å¶é—®é¢˜å€¼: d*

å¼±å¯¹å¶: d* â‰¤ p* (æ€»æ˜¯æˆç«‹)
å¼ºå¯¹å¶: d* = p* (å‡¸ä¼˜åŒ– + Slater æ¡ä»¶æ—¶æˆç«‹)

å¯¹å¶é—´éš™: p* - d* â‰¥ 0
  å‡¸é—®é¢˜ä¸­é—´éš™ä¸º 0 â†’ å¯ä»¥è§£å¯¹å¶é—®é¢˜æ›¿ä»£åŸå§‹é—®é¢˜
  å¯¹å¶é—®é¢˜å¸¸å¸¸æ›´å®¹æ˜“æ±‚è§£ (å¦‚ SVM)
\`\`\`

### åœ¨ ML ä¸­çš„åº”ç”¨

| ML æ–¹æ³• | çº¦æŸä¼˜åŒ–å½¢å¼ |
|---------|-------------|
| SVM | min â€–wâ€–Â² s.t. margin â‰¥ 1 |
| Ridge | min â€–y-Xwâ€–Â² + Î»â€–wâ€–Â² (ç­‰ä»·äºçº¦æŸå½¢å¼) |
| Lasso | min â€–y-Xwâ€–Â² s.t. â€–wâ€–â‚ â‰¤ t |
| ä¿¡æ¯ç“¶é¢ˆ | max I(T;Y) s.t. I(T;X) â‰¤ Ic |
| æœ€å¤§ç†µæ¨¡å‹ | max H(p) s.t. ç‰¹å¾æœŸæœ›åŒ¹é… |`,
  },
  {
    taskTitle: 'Information Theory',
    title: 'ä¿¡æ¯è®ºåœ¨æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨',
    content: `## ä¿¡æ¯è®ºåœ¨æ·±åº¦å­¦ä¹ ä¸­çš„å‰æ²¿åº”ç”¨

### å¯¹æ¯”å­¦ä¹ ä¸ InfoNCE

\`\`\`
å¯¹æ¯”å­¦ä¹ çš„ç›®æ ‡: æœ€å¤§åŒ–æ­£æ ·æœ¬å¯¹çš„äº’ä¿¡æ¯

InfoNCE æŸå¤± (Noise Contrastive Estimation):
  L = -E[log exp(f(x,yâº)/Ï„) / (exp(f(x,yâº)/Ï„) + Î£ exp(f(x,yáµ¢â»)/Ï„))]

  f: ç›¸ä¼¼åº¦å‡½æ•° (é€šå¸¸æ˜¯ä½™å¼¦ç›¸ä¼¼åº¦)
  Ï„: æ¸©åº¦å‚æ•°
  yâº: æ­£æ ·æœ¬
  yáµ¢â»: è´Ÿæ ·æœ¬

InfoNCE æ˜¯äº’ä¿¡æ¯ I(X;Y) çš„ä¸‹ç•Œ!
  I(X;Y) â‰¥ log(N) - L_InfoNCE

åº”ç”¨:
  SimCLR: åŒä¸€å›¾åƒçš„ä¸¤ä¸ªå¢å¼ºè§†å›¾ä½œä¸ºæ­£æ ·æœ¬å¯¹
  CLIP: åŒ¹é…çš„å›¾æ–‡å¯¹ä½œä¸ºæ­£æ ·æœ¬å¯¹
  éŸ³é¢‘: å¯¹æ¯”é¢„æµ‹ç¼–ç  (CPC)
\`\`\`

### çŸ¥è¯†è’¸é¦ä¸­çš„ KL æ•£åº¦

\`\`\`
Teacher è¾“å‡ºè½¯æ ‡ç­¾: p_T = softmax(z_T / T)
Student è¾“å‡º: p_S = softmax(z_S / T)

è’¸é¦æŸå¤±:
  L = Î± Â· CE(y, p_S) + (1-Î±) Â· TÂ² Â· KL(p_T || p_S)

æ¸©åº¦ T çš„ä½œç”¨:
  T=1: æ­£å¸¸ softmax
  T>1: æ›´å¹³æ»‘çš„åˆ†å¸ƒ â†’ æš´éœ²ç±»é—´å…³ç³»
  ä¾‹: çŒ« 0.7, ç‹— 0.2, è½¦ 0.1
      â†’ T=5: çŒ« 0.4, ç‹— 0.35, è½¦ 0.25
      "çŒ«å’Œç‹—çš„ç›¸ä¼¼æ€§"è¿™ç§æš—çŸ¥è¯†è¢«æ”¾å¤§
\`\`\`

### VAE ä¸­çš„ KL æ•£åº¦

\`\`\`
ELBO = E_q[log p(x|z)] - KL(q(z|x) || p(z))
       é‡æ„é¡¹            æ­£åˆ™é¡¹

å½“ q(z|x) = N(Î¼, ÏƒÂ²), p(z) = N(0, I):
KL = -0.5 Â· Î£(1 + log(ÏƒÂ²) - Î¼Â² - ÏƒÂ²)

KL é¡¹çš„ä½œç”¨:
1. çº¦æŸæ½œç©ºé—´æ¥è¿‘æ ‡å‡†æ­£æ€ â†’ å¯ä»¥é‡‡æ ·ç”Ÿæˆ
2. é˜²æ­¢æ½œç©ºé—´åç¼©ä¸ºç¡®å®šæ€§ç¼–ç 
3. æä¾›æ­£åˆ™åŒ–æ•ˆæœ

KL åç¼© (Posterior Collapse):
  å½“è§£ç å™¨å¤ªå¼ºæ—¶ï¼Œq(z|x) â‰ˆ p(z) â†’ æ½œå˜é‡æ²¡æœ‰ç¼–ç ä»»ä½•ä¿¡æ¯
  è§£å†³: KL é€€ç«ã€è‡ªç”±æ¯”ç‰¹ã€Î´-VAE
\`\`\`

### RLHF ä¸­çš„ KL æƒ©ç½š

\`\`\`
RLHF ä¼˜åŒ–ç›®æ ‡:
  max E[R(x,y)] - Î² Â· KL(Ï€_Î¸ || Ï€_ref)

  R(x,y): å¥–åŠ±æ¨¡å‹çš„åˆ†æ•°
  Ï€_Î¸:    å½“å‰ç­–ç•¥
  Ï€_ref:  å‚è€ƒç­–ç•¥ (SFT æ¨¡å‹)
  Î²:      KL æƒ©ç½šç³»æ•°

ä¸ºä»€ä¹ˆéœ€è¦ KL æƒ©ç½š?
  1. é˜²æ­¢å¥–åŠ±é»‘å®¢ (reward hacking): æ‰¾åˆ°å¥–åŠ±æ¨¡å‹çš„æ¼æ´
  2. ä¿æŒè¯­è¨€æµç•…æ€§: ä¸åç¦»é¢„è®­ç»ƒå¤ªè¿œ
  3. ç¨³å®šè®­ç»ƒ: é™åˆ¶ç­–ç•¥æ›´æ–°å¹…åº¦

Î² çš„å½±å“:
  Î² å¤ªå° â†’ å¥–åŠ±é»‘å®¢ã€è¾“å‡ºé€€åŒ–
  Î² å¤ªå¤§ â†’ å‡ ä¹ä¸æ›´æ–°ã€æ— æ³•ä¼˜åŒ–å¥–åŠ±
  é€šå¸¸åŠ¨æ€è°ƒèŠ‚: å½“ KL > target æ—¶å¢å¤§ Î²
\`\`\`

### æ•°æ®å‹ç¼©ä¸æœ€ä¼˜ç¼–ç 

\`\`\`
Shannon ç¼–ç å®šç†:
  æ— æŸå‹ç¼©çš„ç†è®ºä¸‹é™ = ç†µ H(X)

Huffman ç¼–ç : è¾¾åˆ°ç†µç•Œçš„æœ€ä¼˜å‰ç¼€ç 
  é¢‘ç‡é«˜çš„ç¬¦å· â†’ çŸ­ç¼–ç 
  é¢‘ç‡ä½çš„ç¬¦å· â†’ é•¿ç¼–ç 

åœ¨ NLP ä¸­çš„ç±»æ¯”:
  BPE (Byte Pair Encoding) æ˜¯ä¸€ç§è‡ªé€‚åº”ç¼–ç 
  å¸¸è§å­è¯ â†’ å•ä¸ª token (çŸ­ç¼–ç )
  ç½•è§å­è¯ â†’ å¤šä¸ª token (é•¿ç¼–ç )
  è¿™æœ¬è´¨ä¸Šæ˜¯åœ¨åšæ•°æ®å‹ç¼©!

LLM ä¸å‹ç¼©çš„å…³ç³»:
  è®­ç»ƒè¯­è¨€æ¨¡å‹ = å­¦ä¹ æ•°æ®çš„æ¦‚ç‡åˆ†å¸ƒ
  = æ‰¾åˆ°æœ€ä¼˜çš„ç¼–ç æ–¹æ¡ˆ
  = å‹ç¼©æ•°æ®
  Hutter Prize: èƒ½å‹ç¼©ç»´åŸºç™¾ç§‘ = èƒ½ç†è§£è¯­è¨€
\`\`\``,
  },
]

async function main() {
  console.log('Expanding AI åŸºç¡€ notes...\n')

  // Update existing notes
  for (const update of NOTE_UPDATES) {
    const note = await prisma.note.update({
      where: { id: update.id },
      data: { content: update.content },
      include: { task: { select: { title: true } } },
    })
    console.log(`  âœ“ Updated: [${note.task.title}] ${note.title} (${update.content.length} chars)`)
  }

  // Add new notes
  for (const newNote of NEW_NOTES) {
    const task = await prisma.task.findFirst({ where: { title: newNote.taskTitle } })
    if (!task) {
      console.log(`  âœ— Task not found: ${newNote.taskTitle}`)
      continue
    }
    await prisma.note.create({
      data: {
        taskId: task.id,
        title: newNote.title,
        content: newNote.content,
      },
    })
    console.log(`  + New: [${newNote.taskTitle}] ${newNote.title} (${newNote.content.length} chars)`)
  }

  console.log('\nDone! AI åŸºç¡€ notes expanded.')
}

main()
  .catch((e) => { console.error(e); process.exit(1) })
  .finally(() => prisma.$disconnect())
